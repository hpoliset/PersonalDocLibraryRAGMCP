<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Spiritual Library MCP Server - Architecture</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 0;
            text-align: center;
            margin-bottom: 40px;
            border-radius: 10px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .section {
            background: white;
            padding: 30px;
            margin-bottom: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        
        h2 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 1.8em;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
        }
        
        h3 {
            color: #764ba2;
            margin: 20px 0 10px 0;
            font-size: 1.3em;
        }
        
        .architecture-diagram {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border: 2px solid #e9ecef;
        }
        
        .component {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }
        
        .component h4 {
            margin-bottom: 5px;
        }
        
        .file-list {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
        }
        
        .file-list ul {
            list-style-type: none;
            padding-left: 20px;
        }
        
        .file-list li {
            padding: 5px 0;
            border-bottom: 1px solid #e9ecef;
        }
        
        .file-list li:last-child {
            border-bottom: none;
        }
        
        code {
            background: #f8f9fa;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        pre {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 10px 0;
            border: 1px solid #e9ecef;
        }
        
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }
        
        .stat-card h3 {
            color: white;
            font-size: 2em;
            margin: 0;
        }
        
        .stat-card p {
            margin-top: 5px;
            opacity: 0.9;
        }
        
        .flow-diagram {
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: wrap;
            margin: 20px 0;
        }
        
        .flow-item {
            flex: 1;
            min-width: 150px;
            text-align: center;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            margin: 10px;
            border: 2px solid #667eea;
        }
        
        .flow-arrow {
            font-size: 2em;
            color: #667eea;
            margin: 0 10px;
        }
        
        .warning {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            color: #856404;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
        }
        
        .success {
            background: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üîÆ Spiritual Library MCP Server</h1>
            <p>Production-Ready Architecture Documentation (v2.1)</p>
            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px; margin-top: 15px;">
                <strong>‚úÖ FULLY OPERATIONAL</strong> - ARM64 compatible, 768-dim embeddings, all 9 tools working
            </div>
        </header>

        <div class="section">
            <h2>System Overview</h2>
            <p>The Spiritual Library MCP Server is a sophisticated system that enables Claude to access and analyze a personal collection of spiritual books through RAG (Retrieval-Augmented Generation). It combines PDF processing, semantic search, vector storage, and LLM-powered synthesis to provide intelligent responses about spiritual texts.</p>
            
            <div class="stats">
                <div class="stat-card">
                    <h3>XX</h3>
                    <p>Indexed Books</p>
                </div>
                <div class="stat-card">
                    <h3>XX,XXX</h3>
                    <p>Text Chunks</p>
                </div>
                <div class="stat-card">
                    <h3>~XX,XXX</h3>
                    <p>Total Pages</p>
                </div>
                <div class="stat-card">
                    <h3>100%</h3>
                    <p>Local Processing</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Architecture Components</h2>
            
            <div class="architecture-diagram">
                <h3>System Flow</h3>
                <div class="flow-diagram">
                    <div class="flow-item">
                        <strong>Claude Desktop</strong>
                        <p>MCP Client</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>MCP Server</strong>
                        <p>Protocol Handler</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>RAG System</strong>
                        <p>Search & Synthesis</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>Vector DB</strong>
                        <p>ChromaDB</p>
                    </div>
                </div>
            </div>

            <div class="component">
                <h4>1. MCP Complete Server (mcp_complete_server.py) - CURRENT</h4>
                <p>‚úÖ Production MCP server with 9 tools, lazy initialization, and ARM64 compatibility. Proper MCP protocol compliance with serverInfo format and JSON-RPC error handling.</p>
            </div>

            <div class="component">
                <h4>2. Vector Database (ChromaDB) - UPDATED</h4>
                <p>‚úÖ Stores 768-dimensional embeddings using sentence-transformers/all-mpnet-base-v2. Maintains index of text chunks with proper dimension compatibility. ARM64 native dependencies.</p>
            </div>

            <div class="component">
                <h4>3. LLM Integration (Ollama)</h4>
                <p>Uses llama3.3:70b model for synthesis and analysis. Requires ~40GB RAM for optimal performance. All processing happens locally without external API calls.</p>
            </div>

            <div class="component">
                <h4>4. PDF Processing System</h4>
                <p>Processes PDFs from <code>./books/</code> directory. Chunks text into 1000-character segments with 200-character overlap. Automatically categorizes content into practice, energy_work, philosophy, and general.</p>
            </div>
        </div>

        <div class="section">
            <h2>Current File Structure</h2>
            
            <div class="file-list">
                <h3>Production Files (v2.1 - Current)</h3>
                <ul>
                    <li><strong>mcp_complete_server.py</strong> - ‚úÖ Main MCP server (9 tools, ARM64 compatible)</li>
                    <li><strong>shared_rag.py</strong> - ‚úÖ Core RAG with 768-dim embeddings</li>
                    <li><strong>index_monitor.py</strong> - Background indexing service</li>
                    <li><strong>monitor_web_simple.py</strong> - Real-time web dashboard</li>
                    <li><strong>clean_pdfs.py</strong> - Automatic PDF cleaning utility</li>
                    <li><strong>README.md</strong> - Complete deployment guide</li>
                    <li><strong>CLAUDE.md</strong> - Claude Code instructions</li>
                    <li><strong>CHANGELOG.md</strong> - Version history</li>
                    <li><strong>CONTRIBUTING.md</strong> - Developer guidelines</li>
                    <li><strong>PROJECT_STRUCTURE.md</strong> - Architecture documentation</li>
                    <li><strong>LICENSE</strong> - MIT License</li>
                    <li><strong>.github/</strong> - CI/CD and issue templates</li>
                </ul>
            </div>

            <div class="file-list">
                <h3>Data Directories</h3>
                <ul>
                    <li><strong>books/</strong> - PDF library (auto-created)</li>
                    <li><strong>chroma_db/</strong> - Vector database storage (768-dim)</li>
                    <li><strong>venv_mcp/</strong> - ‚úÖ ARM64 Python environment (CURRENT)</li>
                    <li><strong>venv/</strong> - Legacy x86_64 environment</li>
                    <li><strong>backup/</strong> - Legacy/backup files</li>
                    <li><strong>.git/</strong> - Git repository with 3 commits</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üîß Usage Modes - Choose Your Workflow</h2>
            
            <div class="architecture-diagram">
                <h3>Three Flexible Modes</h3>
                
                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin-top: 20px;">
                    <div class="component">
                        <h4>Mode 1: Automatic</h4>
                        <p><strong>For:</strong> Most users</p>
                        <p><strong>How:</strong> Just use Claude normally</p>
                        <p><strong>When indexing happens:</strong> First query after adding books</p>
                        <code>./run.sh</code>
                    </div>
                    
                    <div class="component">
                        <h4>Mode 2: Background</h4>
                        <p><strong>For:</strong> Power users</p>
                        <p><strong>How:</strong> Run monitor service</p>
                        <p><strong>When indexing happens:</strong> Instantly on file changes</p>
                        <p><strong>Control:</strong> Pause/Resume support</p>
                        <code>./index_monitor.sh</code>
                    </div>
                    
                    <div class="component">
                        <h4>Mode 3: Manual</h4>
                        <p><strong>For:</strong> Full control</p>
                        <p><strong>How:</strong> Run when you want</p>
                        <p><strong>When indexing happens:</strong> When you trigger it</p>
                        <code>./run.sh --index-only</code>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>‚úÖ Available MCP Tools (v2.1 - All Working)</h2>
            
            <h3>Search & Analysis Tools</h3>
            <div class="component">
                <h4>search</h4>
                <p>‚úÖ Semantic search with synthesis - returning actual results</p>
                <p><em>Parameters:</em> query, limit, filter_type, synthesize</p>
            </div>

            <div class="component">
                <h4>find_practices</h4>
                <p>Find specific spiritual practices and techniques</p>
                <p><em>Parameter:</em> practice_type</p>
            </div>

            <div class="component">
                <h4>compare_perspectives</h4>
                <p>Compare perspectives from different sources on a topic</p>
                <p><em>Parameter:</em> topic</p>
            </div>

            <h3>Content Tools (v2.1)</h3>
            <div class="component">
                <h4>summarize_book</h4>
                <p>Generate AI summary of an entire book</p>
                <p><em>Parameters:</em> book_name, summary_length (brief/detailed)</p>
            </div>

            <div class="component">
                <h4>extract_quotes</h4>
                <p>Find notable quotes on a specific topic</p>
                <p><em>Parameters:</em> topic, max_quotes</p>
            </div>

            <div class="component">
                <h4>daily_reading</h4>
                <p>Get suggested passages for daily spiritual practice</p>
                <p><em>Parameters:</em> theme, length (short/medium/long)</p>
            </div>

            <div class="component">
                <h4>question_answer</h4>
                <p>Ask direct questions about the teachings</p>
                <p><em>Parameters:</em> question, detail_level (concise/detailed)</p>
            </div>

            <h3>Status Tools</h3>
            <div class="component">
                <h4>library_stats</h4>
                <p>Get comprehensive library statistics</p>
                <p><em>Shows:</em> Books, chunks, categories, indexing status</p>
            </div>

            <div class="component">
                <h4>index_status</h4>
                <p>Get detailed indexing status and pending files</p>
                <p><em>Shows:</em> Current activity, queue, history</p>
            </div>
        </div>


        <div class="section">
            <h2>üöÄ Deployment Options - Hybrid Approach</h2>
            
            <div class="architecture-diagram">
                <h3>Recommended Setup</h3>
                <div class="flow-diagram">
                    <div class="flow-item">
                        <strong>Claude Desktop</strong>
                        <p>Auto-launches MCP server</p>
                    </div>
                    <span class="flow-arrow">+</span>
                    <div class="flow-item">
                        <strong>Ollama Service</strong>
                        <p>Always running</p>
                    </div>
                    <span class="flow-arrow">+</span>
                    <div class="flow-item">
                        <strong>Manual run.sh</strong>
                        <p>For library management</p>
                    </div>
                </div>
            </div>

            <h3>Initial Setup</h3>
            <pre><code># One-time setup
./setup.sh

# Install Ollama as system service
brew services start ollama</code></pre>

            <h3>‚úÖ Current Working Configuration</h3>
            <div class="success">
                <p>Add to Claude Desktop's MCP settings:</p>
                <pre><code>{
  "mcpServers": {
    "spiritual-library": {
      "command": "/Users/KDP/AITools/venv_mcp/bin/python",
      "args": ["/Users/KDP/AITools/mcp_complete_server.py"],
      "env": {
        "PYTHONUNBUFFERED": "1"
      }
    }
  }
}</code></pre>
                <p><strong>‚úÖ WORKING:</strong> ARM64 environment, lazy initialization, all 9 tools operational. Books indexed automatically on first use.</p>
            </div>

            <h3>Option 2: Manual Control (For Library Management)</h3>
            <div class="component">
                <h4>Use ./run.sh when you need to:</h4>
                <ul>
                    <li>See real-time indexing progress</li>
                    <li>Force complete re-indexing</li>
                    <li>Debug issues</li>
                    <li>Add many books at once</li>
                </ul>
                <pre><code># Start manually with progress display
./run.sh</code></pre>
            </div>
        </div>

        <div class="section">
            <h2>üìñ Adding New Books - Workflow</h2>
            
            <div class="architecture-diagram">
                <h3>When You Add New PDFs</h3>
                <div class="flow-diagram">
                    <div class="flow-item">
                        <strong>Add PDFs</strong>
                        <p>Copy to books/</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item" style="background: #fff3cd; border-color: #ffc107;">
                        <strong>Choose Method</strong>
                        <p>Auto or Manual?</p>
                    </div>
                </div>
                
                <div style="display: flex; justify-content: space-around; margin-top: 20px;">
                    <div class="flow-item" style="flex: 1; margin: 10px;">
                        <strong>Automatic</strong>
                        <p>Just use Claude normally</p>
                        <p style="font-size: 0.9em; color: #666;">First query will be slower</p>
                    </div>
                    <div class="flow-item" style="flex: 1; margin: 10px;">
                        <strong>Manual</strong>
                        <p>Run ./run.sh</p>
                        <p style="font-size: 0.9em; color: #666;">See indexing progress</p>
                    </div>
                </div>
            </div>

            <div class="warning">
                <strong>‚ö†Ô∏è Important:</strong> First query after adding books triggers indexing (30s - 5min depending on number of books)
            </div>

            <h3>üîß Automatic PDF Cleaning</h3>
            <div class="success">
                <h4>‚ú® No Manual Intervention Needed!</h4>
                <p>The system automatically handles problematic PDFs via <code>shared_rag.py</code>:</p>
                
                <div class="flow-diagram">
                    <div class="flow-item">
                        <strong>PDF Fails</strong>
                        <p>During indexing</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>Auto Clean</strong>
                        <p>Via Ghostscript</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>Backup Original</strong>
                        <p>to books/originals/</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>Index Cleaned</strong>
                        <p>On next start</p>
                    </div>
                </div>
                
                <p><strong>Automatically fixes:</strong></p>
                <ul>
                    <li>‚úÖ Very large PDFs (>100MB)</li>
                    <li>‚úÖ Complex formatting or embedded objects</li>
                    <li>‚úÖ Corrupted or malformed PDFs</li>
                    <li>‚úÖ Font embedding issues</li>
                </ul>
            </div>
            
            <div class="component">
                <h4>Manual Cleaning (Optional)</h4>
                <p>For specific PDFs or batch processing:</p>
                <pre><code># Force clean specific PDFs
python clean_pdfs.py</code></pre>
                
                <p><strong>Status tracking:</strong> Check <code>failed_pdfs.json</code> for processing history</p>
                
                <div class="warning">
                    <strong>Prerequisite:</strong> Install Ghostscript with <code>brew install ghostscript</code>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>üìö Indexing Process - How It Works</h2>
            
            <div class="architecture-diagram">
                <h3>Indexing Workflow</h3>
                <div class="flow-diagram">
                    <div class="flow-item">
                        <strong>PDF Files</strong>
                        <p>books/*.pdf</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>PyPDF2</strong>
                        <p>Extract Text</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>LangChain</strong>
                        <p>Chunk Text</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>HuggingFace</strong>
                        <p>Create Embeddings</p>
                    </div>
                    <span class="flow-arrow">‚Üí</span>
                    <div class="flow-item">
                        <strong>ChromaDB</strong>
                        <p>Store Vectors</p>
                    </div>
                </div>
            </div>

            <div class="component">
                <h4>üîç Where Indexing Happens</h4>
                <p>The indexing logic is embedded within <code>mcp_final_server.py</code> in the <strong>SpiritualLibraryRAG</strong> class. When the server starts, it automatically:</p>
                <ol>
                    <li>Checks for new PDFs in the <code>books/</code> directory</li>
                    <li>Compares file hashes with <code>chroma_db/book_index.json</code></li>
                    <li>Indexes any new or modified PDFs</li>
                </ol>
            </div>

            <h3>Library Roles & Responsibilities</h3>
            
            <div class="component">
                <h4>1. PyPDF2 - PDF Text Extraction</h4>
                <p><strong>Purpose:</strong> Extracts raw text from PDF files</p>
                <p><strong>Process:</strong> Reads each page and converts to plain text, handling various PDF formats and encodings</p>
                <pre><code># Example usage in the code:
pdf_reader = PyPDF2.PdfReader(pdf_file)
for page in pdf_reader.pages:
    text = page.extract_text()</code></pre>
            </div>

            <div class="component">
                <h4>2. LangChain - Text Processing & Chunking</h4>
                <p><strong>Purpose:</strong> Intelligently splits text into manageable chunks for embedding</p>
                <p><strong>Configuration:</strong></p>
                <ul>
                    <li>Chunk size: 1000 characters</li>
                    <li>Chunk overlap: 200 characters</li>
                    <li>Preserves context across chunk boundaries</li>
                </ul>
                <pre><code># Text splitting configuration:
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)</code></pre>
            </div>

            <div class="component">
                <h4>3. HuggingFace Transformers - Creating Embeddings</h4>
                <p><strong>Purpose:</strong> Converts text chunks into numerical vectors for semantic search</p>
                <p><strong>Model:</strong> sentence-transformers/all-mpnet-base-v2 (CURRENT)</p>
                <p><strong>Why this model:</strong></p>
                <ul>
                    <li>Higher quality embeddings</li>
                    <li>768-dimensional vectors</li>
                    <li>Compatible with existing ChromaDB</li>
                    <li>Better semantic understanding</li>
                </ul>
                <pre><code># Embedding creation:
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True}
)</code></pre>
            </div>

            <div class="component">
                <h4>4. ChromaDB - Vector Storage & Retrieval</h4>
                <p><strong>Purpose:</strong> Stores embeddings and enables fast similarity search</p>
                <p><strong>Features:</strong></p>
                <ul>
                    <li>Persistent storage in <code>chroma_db/</code></li>
                    <li>Metadata filtering (by book, type, etc.)</li>
                    <li>Efficient k-nearest neighbor search</li>
                </ul>
                <pre><code># Vector storage:
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)</code></pre>
            </div>

            <div class="component">
                <h4>5. Ollama + Llama 3.3 - Text Synthesis & Analysis</h4>
                <p><strong>Purpose:</strong> Generates intelligent responses based on retrieved chunks</p>
                <p><strong>Model:</strong> llama3.3:70b (70 billion parameters)</p>
                <p><strong>Usage:</strong></p>
                <ul>
                    <li>Synthesizes information from multiple sources</li>
                    <li>Answers questions based on retrieved context</li>
                    <li>Compares perspectives across different books</li>
                    <li>All processing happens locally via Ollama</li>
                </ul>
                <pre><code># LLM synthesis:
llm = Ollama(model="llama3.3:70b")
response = llm.invoke(
    prompt + "\n\nContext:\n" + retrieved_chunks
)</code></pre>
            </div>
        </div>

        <div class="section">
            <h2>üîÑ Complete Data Flow</h2>
            
            <div class="architecture-diagram">
                <h3>From Query to Response</h3>
                <ol>
                    <li><strong>User Query</strong> ‚Üí Claude Desktop sends query via MCP protocol</li>
                    <li><strong>Query Processing</strong> ‚Üí mcp_final_server.py receives and parses request</li>
                    <li><strong>Embedding Creation</strong> ‚Üí HuggingFace converts query to vector</li>
                    <li><strong>Vector Search</strong> ‚Üí ChromaDB finds similar chunks (cosine similarity)</li>
                    <li><strong>Context Retrieval</strong> ‚Üí Top k chunks retrieved with metadata</li>
                    <li><strong>LLM Synthesis</strong> ‚Üí Llama 3.3 generates response using context</li>
                    <li><strong>Response Delivery</strong> ‚Üí Formatted response sent back to Claude</li>
                </ol>
            </div>

            <div class="warning">
                <strong>‚ö†Ô∏è Indexing Triggers:</strong>
                <ul>
                    <li>First server start (initial indexing)</li>
                    <li>New PDF added to books/ directory</li>
                    <li>Existing PDF modified (hash mismatch)</li>
                    <li>Manual re-indexing (delete chroma_db/book_index.json)</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>Technical Details</h2>
            
            <div class="success">
                <strong>‚úÖ Current Performance:</strong>
                <ul>
                    <li>Startup time: <2 seconds (lazy initialization)</li>
                    <li>Search latency: ~1.75s per query (768-dim embeddings)</li>
                    <li>Memory usage: ~40GB for Ollama LLM + embedding model</li>
                    <li>ARM64 native: All dependencies optimized for Apple Silicon</li>
                    <li>Recent test: "thought pollution" ‚Üí 15 relevant passages found</li>
                </ul>
            </div>

            <div class="warning">
                <strong>‚ö†Ô∏è Important Notes:</strong>
                <ul>
                    <li>First-time indexing can take 10-30 minutes</li>
                    <li>All processing is local - no external API calls</li>
                    <li>Books indexed automatically on first query after adding</li>
                    <li>Git repository ready for collaboration and distribution</li>
                </ul>
            </div>

            <div class="success">
                <strong>‚úÖ System Features:</strong>
                <ul>
                    <li>Semantic search with context-aware results</li>
                    <li>Automatic content categorization</li>
                    <li>Multi-source synthesis capabilities</li>
                    <li>Fact-checking against authoritative texts</li>
                    <li>Chapter outline generation</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üîí Lock Management & Conflict Resolution</h2>
            
            <div class="architecture-diagram">
                <h3>How Lock Conflicts are Prevented</h3>
                
                <div class="component">
                    <h4>File-Based Locking System</h4>
                    <p><strong>Lock File:</strong> /tmp/spiritual_library_index.lock</p>
                    <p><strong>Contains:</strong> Process ID and timestamp</p>
                    <p><strong>Auto-cleanup:</strong> Detects and removes stale locks from dead processes</p>
                </div>
                
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px;">
                    <div class="component">
                        <h4>MCP Server Behavior</h4>
                        <p>When lock is held by another process:</p>
                        <ul style="list-style: none; padding: 10px 0;">
                            <li>‚úÖ Proceeds immediately with existing index</li>
                            <li>‚úÖ No delay for users</li>
                            <li>‚úÖ Logs that another process is indexing</li>
                        </ul>
                    </div>
                    
                    <div class="component">
                        <h4>Background Monitor Behavior</h4>
                        <p>When lock is held by another process:</p>
                        <ul style="list-style: none; padding: 10px 0;">
                            <li>‚è±Ô∏è Schedules retry after delay</li>
                            <li>‚è±Ô∏è 2s delay (normal) or 5s (service mode)</li>
                            <li>‚è±Ô∏è Keeps trying until successful</li>
                        </ul>
                    </div>
                </div>
                
                <div class="warning" style="margin-top: 20px;">
                    <strong>Stale Lock Detection:</strong>
                    <ul>
                        <li>Checks if lock holder process is still alive</li>
                        <li>Considers locks older than 30 minutes as stale</li>
                        <li>Automatically cleans up on next access attempt</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>üñ•Ô∏è Web Monitoring Interface</h2>
            
            <div class="architecture-diagram">
                <h3>Real-Time Status Dashboard</h3>
                <p>Access at <strong>http://localhost:8888</strong> when running <code>monitor_web.py</code></p>
                
                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin-top: 20px;">
                    <div class="component">
                        <h4>Live Information</h4>
                        <ul style="list-style: none; padding: 10px 0;">
                            <li>üìä Current indexing status</li>
                            <li>üìà Progress bar with percentage</li>
                            <li>üìö Library statistics</li>
                            <li>üîí Lock status and health</li>
                            <li>üìÅ Pending files queue</li>
                        </ul>
                    </div>
                    
                    <div class="component">
                        <h4>Interactive Controls</h4>
                        <ul style="list-style: none; padding: 10px 0;">
                            <li>üîÑ Manual index trigger button</li>
                            <li>‚ö° Auto-refresh (2s when indexing)</li>
                            <li>üìù Recent activity log</li>
                            <li>‚ö†Ô∏è Failed PDF alerts</li>
                            <li>üé® Modern, responsive UI</li>
                            <li>‚è∏Ô∏è <strong>NEW:</strong> Pause/Resume buttons</li>
                            <li>üìä <strong>NEW:</strong> Accurate progress tracking</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>üöÄ Service Mode (LaunchAgent)</h2>
            
            <div class="success">
                <h4>Run Index Monitor as System Service</h4>
                <p>For users who want automatic background indexing on system startup:</p>
            </div>
            
            <div class="architecture-diagram">
                <h3>Service Management Commands</h3>
                <pre><code># Install and start service
./install_service.sh

# Check service status and health
./service_status.sh

# Stop and uninstall service
./uninstall_service.sh</code></pre>
                
                <div class="component" style="margin-top: 20px;">
                    <h4>Service Features</h4>
                    <ul style="list-style: none; padding: 10px 0;">
                        <li>üöÄ Starts automatically on system boot</li>
                        <li>üîÑ Auto-restart on failure (30s delay)</li>
                        <li>üéØ Lower priority (nice 10)</li>
                        <li>‚è±Ô∏è Longer retry delays (5s vs 2s)</li>
                        <li>üìù Separate log files for stdout/stderr</li>
                        <li>üõ°Ô∏è Graceful shutdown handling</li>
                        <li>‚è∏Ô∏è <strong>NEW:</strong> Pause/Resume support</li>
                        <li>üìä <strong>NEW:</strong> Real-time progress tracking</li>
                    </ul>
                </div>
            </div>

            <div class="success" style="margin-top: 30px;">
                <h4>üéÆ Pause/Resume Control (NEW)</h4>
                <p>Control indexing without stopping the service:</p>
                <pre><code># Pause indexing (completes current PDF first)
./scripts/pause_indexing.sh

# Resume indexing
./scripts/resume_indexing.sh

# Check comprehensive status
./scripts/indexing_status.sh</code></pre>
                
                <div class="flow-diagram" style="margin-top: 20px;">
                    <div class="flow-item">
                        <strong>Running</strong>
                        <p>Indexing active</p>
                    </div>
                    <span class="flow-arrow">‚è∏Ô∏è</span>
                    <div class="flow-item" style="background: #fff3cd;">
                        <strong>Paused</strong>
                        <p>Queues new files</p>
                    </div>
                    <span class="flow-arrow">‚ñ∂Ô∏è</span>
                    <div class="flow-item">
                        <strong>Resumed</strong>
                        <p>Processes queue</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Troubleshooting</h2>
            
            <h3>Common Issues</h3>
            <ul>
                <li><strong>Import errors:</strong> Ensure virtual environment is activated</li>
                <li><strong>Ollama not found:</strong> Install with <code>brew install ollama</code></li>
                <li><strong>Memory issues:</strong> Ensure sufficient RAM for LLM model</li>
                <li><strong>Search not working:</strong> Check if PDFs are properly indexed in ChromaDB</li>
                <li><strong>Lock conflicts:</strong> Check lock status with <code>./service_status.sh</code></li>
                <li><strong>Service won't start:</strong> Check logs in <code>index_monitor_stderr.log</code></li>
            </ul>

            <h3>Logs Location</h3>
            <ul>
                <li><code>mcp_startup.log</code> - Startup logs</li>
                <li><code>mcp_server_stdout.log</code> - Server output</li>
                <li><code>mcp_server_stderr.log</code> - Error logs</li>
                <li><code>index_monitor_stdout.log</code> - Background monitor output</li>
                <li><code>index_monitor_stderr.log</code> - Background monitor errors</li>
            </ul>
        </div>
    </div>
</body>
</html>